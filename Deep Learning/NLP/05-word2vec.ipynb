{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4834be4-8a7b-4283-bc5f-54255fdd41e8",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54add74a-2549-46b9-9b34-85826ad3b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_helpers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85608a24-a002-48d8-ad19-f04dc4ff3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words= stopwords.words('english')\n",
    "vocabulary_size= 2000\n",
    "embedding_size= 200\n",
    "batch_size= 512\n",
    "generations= 10\n",
    "max_words= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b1af742-4a5b-45f1-8b7e-7e2c121e1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Movies data\n",
    "texts, targets = nlp_helpers.load_movies_data()\n",
    "# Normalize Text\n",
    "texts= nlp_helpers.normalize_text(texts, stop_words)\n",
    "# Crate Word Dictionary\n",
    "word_dic= nlp_helpers.build_dictionary(texts, vocabulary_size)\n",
    "# Reverse Word Dictionary\n",
    "word_dic_rev= dict(zip(word_dic.values(), word_dic.keys()))\n",
    "# Text to Numbers\n",
    "text_data= nlp_helpers.text_to_numbers(texts, word_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a8057c9-8f7c-4d75-9dc2-3e82e9411128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Skip-Grams Embeddings:\n",
      "                    (2000, 200)\n",
      "    CBOW Embeddings:\n",
      "                    (2000, 200)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Embeddings\n",
    "# Skip-Grams\n",
    "file= 'skip-grams-200.pickle'\n",
    "with open(os.path.join('Resources', file), 'rb') as f:\n",
    "    skip_grams_embeddings= pickle.load(f)\n",
    "\n",
    "# CBOW\n",
    "file= 'cbow-200.pickle'\n",
    "with open(os.path.join('Resources', file), 'rb') as f:\n",
    "    cbow_embeddings= pickle.load(f)\n",
    "\n",
    "print(f\"\"\"\n",
    "    Skip-Grams Embeddings:\n",
    "                    {skip_grams_embeddings.shape}\n",
    "    CBOW Embeddings:\n",
    "                    {cbow_embeddings.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1a8ce64-fa1e-40d6-96b7-f9d864a25315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  FIll with 0s sentences with len(words) < max_words\n",
    "text_data = [(row+[0]*max_words)[:max_words] for row in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c3ec8-1ef4-4c3f-b016-3d6c451a24af",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "159ede24-6b3c-4157-beef-2e7038cab49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To tensorflow constants\n",
    "X_const= tf.constant(text_data, dtype= tf.int32)\n",
    "y_const= tf.constant(targets, dtype= tf.float32)\n",
    "embeddings= tf.constant(cbow_embeddings, dtype= tf.float32)\n",
    "# To Tensors\n",
    "X_tensor= tf.data.Dataset.from_tensor_slices(X_const)\n",
    "y_tensor= tf.data.Dataset.from_tensor_slices(targets)\n",
    "# Zip data\n",
    "samples= tf.data.Dataset.zip((X_tensor, y_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "19f9ed33-b1b0-4fc8-a263-53895936701b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 -- Accuracy: 0.5 -- Loss: 7.712477684020996\n",
      "Iteration: 1 -- Accuracy: 0.5 -- Loss: 7.624630928039551\n",
      "Iteration: 2 -- Accuracy: 0.5 -- Loss: 7.624630928039551\n",
      "Iteration: 3 -- Accuracy: 0.5 -- Loss: 7.624630928039551\n",
      "Iteration: 4 -- Accuracy: 0.5 -- Loss: 7.624630928039551\n",
      "Iteration: 5 -- Accuracy: 0.5 -- Loss: 7.712477684020996\n",
      "Iteration: 6 -- Accuracy: 0.5 -- Loss: 7.712477684020996\n",
      "Iteration: 7 -- Accuracy: 0.5 -- Loss: 7.624630928039551\n",
      "Iteration: 8 -- Accuracy: 0.5 -- Loss: 7.624630928039551\n",
      "Iteration: 9 -- Accuracy: 0.5 -- Loss: 7.712477684020996\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "units= 200\n",
    "# First Layer\n",
    "W0= tf.Variable(tf.random.uniform(shape= [units, embedding_size], dtype= tf.float32, minval= 0.0, maxval= 1.0))\n",
    "b0= tf.Variable(tf.zeros(shape= [units, 1], dtype= tf.float32))\n",
    "# Second Layer\n",
    "W1= tf.Variable(tf.random.uniform(shape= [1, units], dtype= tf.float32, minval= 0.0, maxval= 1.0))\n",
    "b1= tf.Variable(tf.zeros(shape= [1, 1], dtype= tf.float32))\n",
    "#\n",
    "# Define Tensorflow Functions\n",
    "#\n",
    "\n",
    "# Function to encode embeddings\n",
    "@tf.function\n",
    "def encode_embeddings(X):\n",
    "    return tf.reduce_mean(tf.nn.embedding_lookup(embeddings, X), axis= 1)\n",
    "\n",
    "@tf.function\n",
    "def model(X):\n",
    "    A0= tf.nn.relu(tf.matmul(W0, X, transpose_b= True) + b0)\n",
    "    A1= tf.nn.sigmoid(tf.matmul(W1, A0) + b1)\n",
    "    return A1\n",
    "\n",
    "# Losss Function\n",
    "@tf.function\n",
    "def loss_function(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.losses.binary_crossentropy(y_true, y_pred))\n",
    "\n",
    "# Optimizer\n",
    "eta= 0.01\n",
    "my_opt= tf.optimizers.legacy.Adam(learning_rate= eta)\n",
    "\n",
    "# Get Score\n",
    "@tf.function\n",
    "def score(X, y, threshold= 0.5):\n",
    "    prediction= tf.cast(model(X) > threshold, dtype= tf.float32)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y, prediction), dtype= tf.float32))\n",
    "\n",
    "# Main lopp\n",
    "for ite in range(generations):\n",
    "    batches= samples.shuffle(buffer_size= len(text_data)).batch(batch_size)\n",
    "    for x_rand, y_rand in batches:\n",
    "        x_encoded= encode_embeddings(x_rand)\n",
    "        y_rand= tf.expand_dims(y_rand, 0)\n",
    "        with tf.GradientTape() as g:\n",
    "            g.watch(W0)\n",
    "            g.watch(b0)\n",
    "            g.watch(W1)\n",
    "            g.watch(b1)\n",
    "\n",
    "            output= model(x_encoded)\n",
    "            loss= loss_function(y_rand, output)\n",
    "        gradients= g.gradient(loss, [W0,b0,W1,b1])\n",
    "        my_opt.apply_gradients(zip(gradients, [W0,b0,W1,b1]))\n",
    "    \n",
    "    if ite % 1 == 0:\n",
    "        X_enc= encode_embeddings(X_const)\n",
    "        y_2d = tf.expand_dims(y_const, 0)\n",
    "        acc= score(X_enc, y_2d)\n",
    "        output= tf.cast(model(X_enc) > 0.5, dtype= tf.float32)\n",
    "        loss= loss_function(y_2d, output)\n",
    "        print(f\"Iteration: {ite} -- Accuracy: {acc.numpy()} -- Loss: {loss.numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
