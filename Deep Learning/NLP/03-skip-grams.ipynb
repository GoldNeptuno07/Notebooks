{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b717c1de",
   "metadata": {},
   "source": [
    "# Skip-Gram\n",
    "\n",
    "- King - man + woman = Queen\n",
    "- Indian Pale Ale - hops + malt = Stout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed6ffde",
   "metadata": {},
   "source": [
    "#### Continuos Bag of Words **CBOW**\n",
    "- If we want to predict an objective word based on context (words surrounding it).\n",
    "\n",
    "#### **Skip-Gram**\n",
    "- If we want to predict the words that surround (context) an objective word.\n",
    "\n",
    "![CBOW & Skip-Gram](Resources/CBOW_Skip-Gram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b5a78",
   "metadata": {},
   "source": [
    "### Download and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b1f1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "import requests\n",
    "import io\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e367cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "embedding_size = 200\n",
    "vocabulary_size = 2000 # 10 000\n",
    "generations = 15000 # 5 000\n",
    "print_loss_every = 1000\n",
    "num_sampled = int(batch_size/2)\n",
    "window_size = 3\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print_valid_every = 2000\n",
    "valid_words = ['cliche', 'love', 'hate', 'silly', 'sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19018861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_movies_data():\n",
    "    # Define paths\n",
    "    save_folder_name = 'movies_data'\n",
    "    pos_file = os.path.join(save_folder_name, 'rt-polarity.pos')\n",
    "    neg_file = os.path.join(save_folder_name, 'rt-polarity.neg')\n",
    "    \n",
    "    if os.path.exists(pos_file) and os.path.exists(neg_file):\n",
    "        ## Get the data from path\n",
    "        pos_data = []\n",
    "        with open(pos_file, 'r') as pos_file_handler:\n",
    "            for row in pos_file_handler:\n",
    "                pos_data.append(row)\n",
    "        neg_data = []\n",
    "        with open(neg_file, 'r') as neg_file_handler:\n",
    "            for row in neg_file_handler:\n",
    "                neg_data.append(row)\n",
    "    else:\n",
    "        # Download data from url\n",
    "        url = \"https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\"\n",
    "        req = requests.get(url)\n",
    "        # Performe Request\n",
    "        if req.ok:\n",
    "            stream_data = io.BytesIO(req.content)\n",
    "            tmp = io.BytesIO() \n",
    "            while True:\n",
    "                s = stream_data.read(16384)\n",
    "                if not s:\n",
    "                    break\n",
    "                tmp.write(s)\n",
    "            stream_data.close()\n",
    "            tmp.seek(0)\n",
    "        else:\n",
    "            raise ConnectionError(f\"Something went wrong. Code: {req.code}\")\n",
    "        # Extract tar File\n",
    "        tar_file = tarfile.open(fileobj= tmp, mode= \"r:gz\")\n",
    "        pos = tar_file.extractfile('rt-polaritydata/rt-polarity.pos')\n",
    "        neg = tar_file.extractfile('rt-polaritydata/rt-polarity.neg')\n",
    "        # Get positive reviews\n",
    "        pos_data = []\n",
    "        for line in pos:\n",
    "            pos_data.append(line.decode(\"ISO-8859-1\").encode('ascii', errors= 'ignore').decode())\n",
    "        # Get negative reviews\n",
    "        neg_data = []\n",
    "        for line in neg:\n",
    "            neg_data.append(line.decode('ISO-8859-1').encode('ascii', errors= 'ignore').decode())\n",
    "        tar_file.close()\n",
    "        # Save data\n",
    "        os.makedirs(save_folder_name, exist_ok= True)\n",
    "        with open(pos_file, 'w') as pos_file_handler:\n",
    "            pos_file_handler.write(''.join(pos_data))\n",
    "        with open(neg_file, 'w') as neg_file_handler:\n",
    "            neg_file_handler.write(''.join(neg_data))\n",
    "    texts = pos_data + neg_data\n",
    "    target = [1]*len(pos_data) + [0]*len(neg_data)\n",
    "    return (texts, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91a700e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, target = load_movies_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b21826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6985498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63545e",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9171b524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(texts, stop):\n",
    "    texts = [x.lower() for x in texts] # To lower case\n",
    "    texts = [re.findall(pattern= \"[a-z]+\", string= x) for x in texts] # Remove Numbers and Punctuation marks\n",
    "    texts = [' '.join([x for x in row if x not in stop]) for row in texts ]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9822ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = normalize_text(texts, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcd3392a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'without heavy handedness dong provides perspective intelligent grasp human foibles contradictions'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4566381d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10662"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "706674cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target = [target[i] for i, x in enumerate(texts) if len(x.split()) > 2]\n",
    "texts = [x for x in texts if len(x.split()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae68ef1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10425"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a872ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(sentences, vocabulary_size):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        words.extend(sentence.split())\n",
    "    count= [['RARE', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    word_dict = {}\n",
    "    for word, word_count in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ab1920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = build_dictionary(texts, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0e18f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b089a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_numbers(sentences, word_dict):\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        sentence_data = []\n",
    "        for word in sentence.split():\n",
    "            if word in word_dict:\n",
    "                word_idx = word_dict[word]\n",
    "            else:\n",
    "                word_idx = 0\n",
    "            sentence_data.append(word_idx)\n",
    "        data.append(sentence_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abc338b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_dict_rev = dict(zip(word_dict.values(), word_dict.keys())) # {idx: word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c137df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[439, 0, 0, 754, 30, 0, 135, 17, 0, 7, 0, 1343, 0, 0, 0, 1569, 0, 799, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = text_to_numbers(texts, word_dict)\n",
    "text_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e15d255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[986, 27, 938, 209, 371]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_examples = [word_dict[x] for x in valid_words]\n",
    "valid_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ae86c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data(sentences, batch_size, window_size, method= 'skip_gram'):\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    while len(batch_data) < batch_size:\n",
    "        rand_idx = np.random.choice(len(sentences))\n",
    "        rand_sentences = sentences[rand_idx]\n",
    "        window_seq = [rand_sentences[max(ix-window_size, 0):ix+window_size+1]\n",
    "                  for ix, x in enumerate(rand_sentences)]\n",
    "        label_idx = [ix if ix < window_size else window_size for ix, x in enumerate(window_seq)]\n",
    "        if method == 'skip_gram':\n",
    "            batch_and_labels = [(x[y], x[:y]+x[y+1:]) for x,y in zip(window_seq, label_idx)]\n",
    "            tuple_data = [(x,y_) for x, y in batch_and_labels for y_ in y]\n",
    "        else:\n",
    "            raise ValueError(f'Invalid Method {method}.')\n",
    "        batch, labels = [list(x) for x in zip(*tuple_data)]\n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array(label_data))\n",
    "    return (batch_data, label_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f590ce-a792-43bd-acab-569818ec9ca0",
   "metadata": {},
   "source": [
    "### Training the Skip-Grams\n",
    "Layers:\n",
    "- input_layer. $[n_{words},~~ batch_n]$\n",
    "- hidden layer. $[n_{embeddings}, ~~ batch_n]$\n",
    "- output layer. $[n_{words},~~ batch_n]$\n",
    "\n",
    "Weights:\n",
    "* input  $~=>~$  hidden (**embeddings**). $[n_{embeddings}, ~~ n_{words]}]$\n",
    "* hidden $~=>~$  output. $[n_{words}, ~~ n_{embeddings}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb3e4b25-8b22-4d33-9116-6c74388fa29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 1000 -- Loss: 1.3687118291854858\n",
      "Iterations: 2000 -- Loss: 1.2327297925949097\n",
      "Word: cliche Similarity: RARE - movie - film - sense - like - funny - sounds - least - kind - really - \n",
      "Word: love Similarity: RARE - movie - film - really - funny - like - sounds - could - terrific - george - \n",
      "Word: hate Similarity: RARE - film - movie - sense - like - kind - sounds - funny - could - read - \n",
      "Word: silly Similarity: RARE - movie - film - end - sounds - read - really - sense - kind - funny - \n",
      "Word: sad Similarity: RARE - read - movie - kind - funny - sense - science - like - george - lacks - \n",
      "Iterations: 3000 -- Loss: 1.1505544185638428\n",
      "Iterations: 4000 -- Loss: 1.5082716941833496\n",
      "Word: cliche Similarity: RARE - movie - film - sense - like - funny - sounds - least - kind - really - \n",
      "Word: love Similarity: RARE - movie - film - really - funny - like - sounds - could - terrific - george - \n",
      "Word: hate Similarity: RARE - film - movie - sense - like - kind - sounds - funny - could - read - \n",
      "Word: silly Similarity: RARE - movie - film - end - sounds - read - really - sense - kind - funny - \n",
      "Word: sad Similarity: RARE - read - movie - kind - funny - sense - science - like - george - lacks - \n",
      "Iterations: 5000 -- Loss: 1.3896833658218384\n",
      "Iterations: 6000 -- Loss: 1.3443628549575806\n",
      "Word: cliche Similarity: RARE - movie - film - sense - like - funny - sounds - least - kind - really - \n",
      "Word: love Similarity: RARE - movie - film - really - funny - like - sounds - could - terrific - george - \n",
      "Word: hate Similarity: RARE - film - movie - sense - like - kind - sounds - funny - could - read - \n",
      "Word: silly Similarity: RARE - movie - film - end - sounds - read - really - sense - kind - funny - \n",
      "Word: sad Similarity: RARE - read - movie - kind - funny - sense - science - like - george - lacks - \n",
      "Iterations: 7000 -- Loss: 1.2562187910079956\n",
      "Iterations: 8000 -- Loss: 1.313084363937378\n",
      "Word: cliche Similarity: RARE - movie - film - sense - like - funny - sounds - least - kind - really - \n",
      "Word: love Similarity: RARE - movie - film - really - funny - like - sounds - could - terrific - george - \n",
      "Word: hate Similarity: RARE - film - movie - sense - like - kind - sounds - funny - could - read - \n",
      "Word: silly Similarity: RARE - movie - film - end - sounds - read - really - sense - kind - funny - \n",
      "Word: sad Similarity: RARE - read - movie - kind - funny - sense - science - like - george - lacks - \n",
      "Iterations: 9000 -- Loss: 1.375411868095398\n",
      "Iterations: 10000 -- Loss: 1.430403232574463\n",
      "Word: cliche Similarity: RARE - movie - film - sense - like - funny - sounds - least - kind - really - \n",
      "Word: love Similarity: RARE - movie - film - really - funny - like - sounds - could - terrific - george - \n",
      "Word: hate Similarity: RARE - film - movie - sense - like - kind - sounds - funny - could - read - \n",
      "Word: silly Similarity: RARE - movie - film - end - sounds - read - really - sense - kind - funny - \n",
      "Word: sad Similarity: RARE - read - movie - kind - funny - sense - science - like - george - lacks - \n",
      "Iterations: 11000 -- Loss: 1.4999744892120361\n",
      "Iterations: 12000 -- Loss: 1.2277835607528687\n",
      "Word: cliche Similarity: RARE - movie - film - sense - like - funny - sounds - least - kind - really - \n",
      "Word: love Similarity: RARE - movie - film - really - funny - like - sounds - could - terrific - george - \n",
      "Word: hate Similarity: RARE - film - movie - sense - like - kind - sounds - funny - could - read - \n",
      "Word: silly Similarity: RARE - movie - film - end - sounds - read - really - sense - kind - funny - \n",
      "Word: sad Similarity: RARE - read - movie - kind - funny - sense - science - like - george - lacks - \n",
      "Iterations: 13000 -- Loss: 1.5523275136947632\n",
      "Iterations: 14000 -- Loss: 1.3757649660110474\n",
      "Word: cliche Similarity: RARE - movie - film - sense - like - funny - sounds - least - kind - really - \n",
      "Word: love Similarity: RARE - movie - film - really - funny - like - sounds - could - terrific - george - \n",
      "Word: hate Similarity: RARE - film - movie - sense - like - kind - sounds - funny - could - read - \n",
      "Word: silly Similarity: RARE - movie - film - end - sounds - read - really - sense - kind - funny - \n",
      "Word: sad Similarity: RARE - read - movie - kind - funny - sense - science - like - george - lacks - \n",
      "Iterations: 15000 -- Loss: 1.3540725708007812\n"
     ]
    }
   ],
   "source": [
    "# Weights / Bias\n",
    "embeddings = tf.Variable(tf.random.uniform(shape= [embedding_size, vocabulary_size], minval= 0, maxval= 1, dtype= tf.float32))\n",
    "b0 = tf.Variable(tf.zeros(shape= [embedding_size, 1], dtype= tf.float32))\n",
    "W1 = tf.Variable(tf.random.uniform(shape= [vocabulary_size, embedding_size], minval= 0, maxval= 1, dtype= tf.float32))\n",
    "b1 = tf.Variable(tf.zeros(shape= [vocabulary_size, 1], dtype= tf.float32))\n",
    "\n",
    "# Identity Matrix\n",
    "identity = tf.linalg.diag(tf.ones(shape= [vocabulary_size], dtype= tf.float32))\n",
    "# Function to one_hot sampes\n",
    "def one_hot(samples, transpose= False):\n",
    "    encoded = tf.nn.embedding_lookup(identity, samples) \n",
    "    if transpose:\n",
    "        return tf.transpose(encoded)\n",
    "    return encoded\n",
    "\n",
    "# Neural Network\n",
    "@tf.function\n",
    "def model(X):\n",
    "    A0 = tf.nn.relu(tf.matmul(embeddings, X) + b0)\n",
    "    A1 = tf.nn.softmax(tf.matmul(W1, A0) + b1, axis= 0)\n",
    "    return A1\n",
    "\n",
    "# Loss Function\n",
    "@tf.function\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_pred_t = tf.transpose(y_pred)\n",
    "    return tf.reduce_sum(tf.losses.categorical_crossentropy(y_true, y_pred_t)) / len(y_pred)\n",
    "\n",
    "# Compute the Similarity Between validation words and all the vocabulary words\n",
    "@tf.function\n",
    "def compute_similarity(embeddings, val_idx, topk= 10):\n",
    "    # Normalize Embeddings\n",
    "    embeddings_t = tf.transpose(embeddings)\n",
    "    norm = tf.math.sqrt(tf.reduce_sum(tf.square(embeddings_t), 1, keepdims= True))\n",
    "    norm_embeddings = embeddings_t / norm # n_samples, 200\n",
    "    # Get Validation words\n",
    "    validation_words= tf.nn.embedding_lookup(norm_embeddings, val_idx) # n_samples, 200\n",
    "    # Dot Product\n",
    "    cos_similarity = tf.matmul(validation_words, norm_embeddings, transpose_b= True)\n",
    "    # Get top K words\n",
    "    values_K, idx_K = tf.nn.top_k(-cos_similarity, k= topk, )\n",
    "    return idx_K\n",
    "\n",
    "# Optimizer\n",
    "eta = 0.1\n",
    "my_opt = tf.optimizers.legacy.Adam(learning_rate= eta)\n",
    "\n",
    "# Main Loop\n",
    "loss_vect = []\n",
    "for ite in range(1, generations+1):\n",
    "    X_idx, y_idx = generate_batch_data(text_data, batch_size, window_size, method= 'skip_gram')\n",
    "    X_input= tf.constant(one_hot(X_idx, transpose= True), dtype= tf.float32)\n",
    "    y_input= tf.constant(one_hot(y_idx), dtype= tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(embeddings)\n",
    "        g.watch(b0)\n",
    "        g.watch(W1)\n",
    "        g.watch(b1)\n",
    "\n",
    "        output= model(X_input)\n",
    "        loss= loss_function(y_input, output)\n",
    "    gradients= g.gradient(loss, [embeddings, b0, W1, b1])\n",
    "    my_opt.apply_gradients(zip(gradients, [embeddings, b0, W1, b1]))\n",
    "\n",
    "    if ite % print_loss_every == 0:\n",
    "        print(f\"Iterations: {ite} -- Loss: {loss.numpy()}\")\n",
    "    if ite % print_valid_every == 0:\n",
    "        similarity_idx = compute_similarity(embeddings, valid_examples, topk= 10)\n",
    "        for idx, val in enumerate(valid_words):\n",
    "            print(f'Word: {val} Similarity: ', end= '')\n",
    "            for k in similarity_idx[idx].numpy():\n",
    "                print(f'{word_dict_rev[k]}', end= ' - ')\n",
    "            print()\n",
    "    loss_vect.append(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a53e3e7-a2b9-4c81-95c3-9d267e1cbcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vects = tf.transpose(embeddings).numpy() # (vocabulary_size, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8f531e1e-2ade-46aa-a243-1af3ce1ad2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sad'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# king - man + women\n",
    "queen_vect = word_vects[word_dict['sad']] + word_vects[word_dict['happy']] + word_vects[word_dict['love']] # (600,)\n",
    "queen = queen_vect.reshape(1,1,queen_vect.shape[0]) # (1,1,embeddings)\n",
    "\n",
    "# L2 Norm\n",
    "l2_norm = np.power(queen-word_vects, 2)\n",
    "sum_l2_norm = np.sum(l2_norm, axis= 2) # (1,10 000)\n",
    "\n",
    "# Get Nearest Vector\n",
    "idx = np.argmin(sum_l2_norm)\n",
    "word_dict_rev[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c026cfc-ec8c-45d9-a095-76af665648e2",
   "metadata": {},
   "source": [
    "#### Save the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "537aa700-f2d8-4d9d-a424-99dd44fca86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0840adb2-5ce3-4f14-978a-294d414713e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Embeddings\n",
    "with open(os.path.join('Resources', 'skip-grams-200.pickle'), 'wb') as f:\n",
    "    pickle.dump(word_vects, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
